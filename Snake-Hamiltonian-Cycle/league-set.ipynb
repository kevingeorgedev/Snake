{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices 1\n",
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print ('Available devices', T.cuda.device_count())\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "T.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('match_data_v5.csv').drop('last', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['blueWin', 'matchID'], axis=1)\n",
    "y = df['blueWin']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = T.tensor(X_train.values, dtype=T.float32, device=device)\n",
    "X_test = T.tensor(X_test.values, dtype=T.float32, device=device)\n",
    "y_train = T.tensor(y_train.values, dtype=T.float32, device=device).view(-1, 1)\n",
    "y_test = T.tensor(y_test.values, dtype=T.float32, device=device).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Create DataLoader instances for training and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1     = nn.Linear(in_features=27, out_features=512)\n",
    "        self.norm1   = nn.BatchNorm1d(512, momentum=0.9)\n",
    "\n",
    "        self.fc2     = nn.Linear(in_features=512, out_features=256)\n",
    "        self.norm2   = nn.BatchNorm1d(256, momentum=0.8)\n",
    "\n",
    "        self.fc3     = nn.Linear(in_features=256, out_features=128)\n",
    "        self.norm3   = nn.BatchNorm1d(128, momentum=0.7)\n",
    "\n",
    "        self.fc4     = nn.Linear(in_features=128, out_features=32)\n",
    "        self.norm4   = nn.BatchNorm1d(32, momentum=0.7)\n",
    "\n",
    "        self.act     = nn.ReLU()\n",
    "        self.output  = nn.Linear(in_features=32, out_features=1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.norm1(self.fc1(x)))\n",
    "        x = self.act(self.norm2(self.fc2(x)))\n",
    "        x = self.act(self.norm3(self.fc3(x)))\n",
    "        x = self.act(self.norm4(self.fc4(x)))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 Training Loss: 76.51149\n",
      "epoch=1 Training Loss: 75.75999\n",
      "epoch=2 Training Loss: 75.61533\n",
      "epoch=3 Training Loss: 75.33131\n",
      "epoch=4 Training Loss: 75.26352\n",
      "epoch=5 Training Loss: 75.07919\n",
      "epoch=6 Training Loss: 75.20025\n",
      "epoch=7 Training Loss: 75.07555\n",
      "epoch=8 Training Loss: 75.22798\n",
      "epoch=9 Training Loss: 74.97421\n",
      "epoch=10 Training Loss: 75.1411\n",
      "epoch=11 Training Loss: 74.78207\n",
      "epoch=12 Training Loss: 74.73398\n",
      "epoch=13 Training Loss: 74.87786\n",
      "epoch=14 Training Loss: 74.88714\n",
      "epoch=15 Training Loss: 74.73016\n",
      "epoch=16 Training Loss: 74.57953\n",
      "epoch=17 Training Loss: 74.76304\n",
      "epoch=18 Training Loss: 74.8326\n",
      "epoch=19 Training Loss: 74.70199\n",
      "epoch=20 Training Loss: 74.66279\n",
      "epoch=21 Training Loss: 74.55357\n",
      "epoch=22 Training Loss: 74.51359\n",
      "epoch=23 Training Loss: 74.43455\n",
      "epoch=24 Training Loss: 74.49512\n",
      "epoch=25 Training Loss: 74.27851\n",
      "epoch=26 Training Loss: 74.37236\n",
      "epoch=27 Training Loss: 74.21251\n",
      "epoch=28 Training Loss: 74.25828\n",
      "epoch=29 Training Loss: 74.08119\n",
      "epoch=30 Training Loss: 74.02371\n",
      "epoch=31 Training Loss: 74.04645\n",
      "epoch=32 Training Loss: 74.18537\n",
      "epoch=33 Training Loss: 73.88588\n",
      "epoch=34 Training Loss: 73.90189\n",
      "epoch=35 Training Loss: 73.69787\n",
      "epoch=36 Training Loss: 73.83512\n",
      "epoch=37 Training Loss: 73.66102\n",
      "epoch=38 Training Loss: 73.70954\n",
      "epoch=39 Training Loss: 73.73817\n",
      "epoch=40 Training Loss: 73.66673\n",
      "epoch=41 Training Loss: 73.30277\n",
      "epoch=42 Training Loss: 73.47907\n",
      "epoch=43 Training Loss: 73.07472\n",
      "epoch=44 Training Loss: 73.4063\n",
      "epoch=45 Training Loss: 73.08747\n",
      "epoch=46 Training Loss: 73.05565\n",
      "epoch=47 Training Loss: 73.00303\n",
      "epoch=48 Training Loss: 72.8426\n",
      "epoch=49 Training Loss: 73.09263\n",
      "epoch=50 Training Loss: 72.5182\n",
      "epoch=51 Training Loss: 72.61804\n",
      "epoch=52 Training Loss: 72.4336\n",
      "epoch=53 Training Loss: 72.36216\n",
      "epoch=54 Training Loss: 72.22696\n",
      "epoch=55 Training Loss: 71.93323\n",
      "epoch=56 Training Loss: 72.0114\n",
      "epoch=57 Training Loss: 71.99604\n",
      "epoch=58 Training Loss: 71.4439\n",
      "epoch=59 Training Loss: 71.48264\n",
      "epoch=60 Training Loss: 71.66891\n",
      "epoch=61 Training Loss: 71.2227\n",
      "epoch=62 Training Loss: 71.42278\n",
      "epoch=63 Training Loss: 71.10777\n",
      "epoch=64 Training Loss: 71.06689\n",
      "epoch=65 Training Loss: 70.85677\n",
      "epoch=66 Training Loss: 70.5598\n",
      "epoch=67 Training Loss: 70.28766\n",
      "epoch=68 Training Loss: 69.87179\n",
      "epoch=69 Training Loss: 70.08086\n",
      "epoch=70 Training Loss: 69.90433\n",
      "epoch=71 Training Loss: 69.71192\n",
      "epoch=72 Training Loss: 69.58497\n",
      "epoch=73 Training Loss: 69.34438\n",
      "epoch=74 Training Loss: 69.14715\n",
      "epoch=75 Training Loss: 68.9942\n",
      "epoch=76 Training Loss: 68.66397\n",
      "epoch=77 Training Loss: 68.6465\n",
      "epoch=78 Training Loss: 68.54046\n",
      "epoch=79 Training Loss: 68.30326\n",
      "epoch=80 Training Loss: 67.99566\n",
      "epoch=81 Training Loss: 68.057\n",
      "epoch=82 Training Loss: 68.16575\n",
      "epoch=83 Training Loss: 67.76499\n",
      "epoch=84 Training Loss: 67.35875\n",
      "epoch=85 Training Loss: 67.30053\n",
      "epoch=86 Training Loss: 67.04071\n",
      "epoch=87 Training Loss: 66.70655\n",
      "epoch=88 Training Loss: 66.92498\n",
      "epoch=89 Training Loss: 66.38025\n",
      "epoch=90 Training Loss: 66.05181\n",
      "epoch=91 Training Loss: 66.60565\n",
      "epoch=92 Training Loss: 66.11652\n",
      "epoch=93 Training Loss: 65.846\n",
      "epoch=94 Training Loss: 66.2719\n",
      "epoch=95 Training Loss: 65.38136\n",
      "epoch=96 Training Loss: 65.52232\n",
      "epoch=97 Training Loss: 65.23739\n",
      "epoch=98 Training Loss: 64.76512\n",
      "epoch=99 Training Loss: 65.28955\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "model = Model().to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model( x_batch )\n",
    "        loss = criterion( logits, y_batch )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch={epoch} Training Loss: {round(total_loss, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09416960961045628"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "predictions_np = model( X_test ).detach().round().cpu().numpy()\n",
    "r2_score(predictions_np, y_test.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
